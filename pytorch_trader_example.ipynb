{
 "cells": [
  {
   "source": [
    "# Pytorch Example\n",
    "\n",
    "This notebook walks through essentially **all** the major steps of the submission process. The only step you have to do is create and zip up the files (main.py, agent.py, and any models or other files you need) and submit them to our website.\n",
    "\n",
    "Specifically, this notebook will...\n",
    "\n",
    "- Load train.csv into a pandas dataframe\n",
    "- Train a RL agent using our custom gym environment* \n",
    "- Save model to disk \n",
    "- Show how to write an agent.py file to use model (see sample agent.py for details)\n",
    "- Show how to write a main.py file to take row as stdin and output (Action, frac) as stdout\n",
    "\n",
    "This notebook does not complete the final steps, namely...\n",
    "\n",
    "- Actually create agent.py \n",
    "- Actually create main.py\n",
    "- Zip agent.py, main.py, and any model files together and submit on \\<insert website url\\>\n",
    "- Note your score and try again!\n",
    "\n",
    "*See util.py or the deep_stock_trader_custom_environment notebook for more details\n",
    "\n",
    "---\n",
    "\n",
    "The model trained here is super basic (and the code has a nasty buy somewhere causing it to only HOLD when evaluated...). Attend Josiah's RL course to learn more. You can also start playing with the network's architecture and the training hyperparameters. Maybe adjust the reward function in the DeepStockTraderEnv. Are there ways to include context/memory? So much to explore!\n",
    "\n",
    "***PLEASE SEE THE END OF THIS NOTEBOOK WHICH OUTLINES HOW TO SUBMIT TO OUR WEBSITE!!***\n",
    "\n",
    "---\n",
    "\n",
    "Good luck, <br>\n",
    "Seth Hamilton | TAMU Datathon R&D"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from util import Action, DeepStockTraderEnv\n",
    "from collections import namedtuple  \n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display"
   ]
  },
  {
   "source": [
    "## Define the network"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TraderNetwork(nn.Module):\n",
    "    def __init__(self, row_size):\n",
    "        super(TraderNetwork, self).__init__()\n",
    "\n",
    "        self.size = row_size\n",
    "\n",
    "        self.fc1 = nn.Linear(row_size, row_size*2)\n",
    "        self.fc2 = nn.Linear(row_size*2, row_size)\n",
    "        self.fc3 = nn.Linear(row_size, 3)\n",
    "\n",
    "    def forward(self, t):\n",
    "        # Have the tensor \"flow\" through the network\n",
    "        t = F.relu(self.fc1(t))\n",
    "        t = F.relu(self.fc2(t))\n",
    "        t = F.relu(self.fc3(t))\n",
    "\n",
    "        return t"
   ]
  },
  {
   "source": [
    "## Define helper classes\n",
    "\n",
    "Much of the following code was copied from DeepLizard.com's RL pytorch course. I highly recommend it. \n",
    "\n",
    "Check it out here: [RL pytorch course](https://deeplizard.com/learn/playlist/PLZbbT5o_s2xoWNVdDudn51XM8lOuZ_Njv)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tensors(experiences):\n",
    "    # Convert batch of Experiences to Experience of batches\n",
    "    batch = Experience(*zip(*experiences))\n",
    "\n",
    "    t1 = torch.cat(batch.state)\n",
    "    t2 = torch.cat(batch.action)\n",
    "    t3 = torch.cat(batch.reward)\n",
    "    t4 = torch.cat(batch.next_state)\n",
    "\n",
    "    return (t1,t2,t3,t4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QValues():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    @staticmethod\n",
    "    def get_current(policy_net, states, actions):\n",
    "        return policy_net(states).gather(dim=1, index=actions)\n",
    "\n",
    "    @staticmethod        \n",
    "    def get_next(target_net, next_states):  \n",
    "        return target_net(next_states).max(dim=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = namedtuple(\n",
    "    'Experience',\n",
    "    ('state', 'action', 'next_state', 'reward')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.push_count = 0\n",
    "\n",
    "    def push(self, experience):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(experience)\n",
    "        else:\n",
    "            self.memory[self.push_count % self.capacity] = experience\n",
    "        self.push_count += 1\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def can_provide_sample(self, batch_size):\n",
    "        return len(self.memory) >= batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyStrategy():\n",
    "    def __init__(self, start, end, decay):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.decay = decay\n",
    "\n",
    "    def get_exploration_rate(self, current_step):\n",
    "        return self.end + (self.start - self.end) * \\\n",
    "            math.exp(-1. * current_step * self.decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingAgent:\n",
    "    def __init__(self, strategy, num_actions, device):\n",
    "        self.current_step = 0\n",
    "        self.strategy = strategy\n",
    "        self.num_actions = num_actions\n",
    "        self.device = device\n",
    "\n",
    "    def select_action(self, state, policy_net):\n",
    "        rate = self.strategy.get_exploration_rate(self.current_step)\n",
    "        self.current_step += 1\n",
    "\n",
    "        if rate > random.random():\n",
    "            return random.randrange(self.num_actions) # explore      \n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                return policy_net(state).argmax(dim=0).item() # exploit    \n"
   ]
  },
  {
   "source": [
    "## Environment/Data setup"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/mystery_stock_daily_train.csv\")\n",
    "env = DeepStockTraderEnv(df)"
   ]
  },
  {
   "source": [
    "## Hyperparameters and Training Loop"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters (Feel free to tune)\n",
    "batch_size = 8\n",
    "gamma = 0.9\n",
    "eps_start = 1\n",
    "eps_end = 0.01\n",
    "eps_decay = 0.001\n",
    "target_update = 10\n",
    "memory_size = 100000\n",
    "lr = 0.001\n",
    "n_episodes = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_moving_average(period, values):\n",
    "    values = torch.tensor(values, dtype=torch.float)\n",
    "    if len(values) >= period:\n",
    "        moving_avg = values.unfold(dimension=0, size=period, step=1) \\\n",
    "            .mean(dim=1).flatten(start_dim=0)\n",
    "        moving_avg = torch.cat((torch.zeros(period-1), moving_avg))\n",
    "        return moving_avg.numpy()\n",
    "    else:\n",
    "        moving_avg = torch.zeros(len(values))\n",
    "        return moving_avg.numpy()\n",
    "\n",
    "def plot(values, moving_avg_period):\n",
    "    plt.clf()        \n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Money USD')\n",
    "    plt.plot(values)\n",
    "    plt.plot(get_moving_average(moving_avg_period, values))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Other important setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "strategy = EpsilonGreedyStrategy(eps_start, eps_end, eps_decay)\n",
    "t_agent = TrainingAgent(strategy, env.action_space.n, device)\n",
    "memory = ReplayMemory(memory_size)\n",
    "\n",
    "policy_net = TraderNetwork(env.row_size).to(device, dtype=torch.double)\n",
    "target_net = TraderNetwork(env.row_size).to(device, dtype=torch.double)\n",
    "\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.Adam(params=policy_net.parameters(), lr=lr)\n",
    "\n",
    "episode_values = []\n",
    "max_value = 0\n",
    "\n",
    "# TRAINING LOOP\n",
    "for episode in range(n_episodes):\n",
    "    state = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.double, device=device)\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "    plot(episode_values, 100)\n",
    "    \n",
    "    timestep = 0\n",
    "    while True:\n",
    "        action = t_agent.select_action(state, policy_net)\n",
    "\n",
    "        next_state, reward, done, info = env.step(Action(action))\n",
    "        next_state = torch.tensor(next_state, dtype=torch.double, device=device)\n",
    "\n",
    "        action_t = torch.tensor([action]).unsqueeze(0)\n",
    "        reward_t = torch.tensor([reward]).unsqueeze(0)\n",
    "\n",
    "        memory.push(Experience(state.unsqueeze(0), action_t, next_state.unsqueeze(0), reward_t))\n",
    "        state = next_state\n",
    "\n",
    "        if memory.can_provide_sample(batch_size):\n",
    "            experiences = memory.sample(batch_size)\n",
    "            states, actions, rewards, next_states = extract_tensors(experiences)\n",
    "\n",
    "            current_q_values = QValues.get_current(policy_net, states, actions)\n",
    "            next_q_values = QValues.get_next(target_net, next_states)\n",
    "            target_q_values = (next_q_values * gamma) + rewards\n",
    "\n",
    "            loss = F.mse_loss(current_q_values, target_q_values.unsqueeze(1))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if done:\n",
    "            total = env.total(timestep=-1, open=False)\n",
    "            episode_values.append(total)\n",
    "            if total > max_value:\n",
    "                max_value = total\n",
    "                torch.save(policy_net.state_dict(), \"./example_model.pt\")\n",
    "            break\n",
    "\n",
    "        timestep += 1\n",
    "\n",
    "    if episode % target_update == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())"
   ]
  },
  {
   "source": [
    "## Sample Submission Construction\n",
    "\n",
    "Remember: You need to submit a zip file containing\n",
    "- main.py\n",
    "- agent.py\n",
    "- example_model.pt (or the name of your model file)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Example agent.py"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np\n",
    "from enum import IntEnum\n",
    "\n",
    "class Action(IntEnum):\n",
    "    BUY = 0\n",
    "    SELL = 1\n",
    "    HOLD = 2\n",
    "\n",
    "# You need to include any network definitions\n",
    "class TraderNetwork(nn.Module):\n",
    "    def __init__(self, row_size):\n",
    "        super(TraderNetwork, self).__init__()\n",
    "\n",
    "        self.size = row_size\n",
    "\n",
    "        self.fc1 = nn.Linear(row_size, row_size*2)\n",
    "        self.fc2 = nn.Linear(row_size*2, row_size)\n",
    "        self.fc3 = nn.Linear(row_size, 3)\n",
    "\n",
    "    def forward(self, t):\n",
    "        # Have the tensor \"flow\" through the network\n",
    "        t = F.relu(self.fc1(t))\n",
    "        t = F.relu(self.fc2(t))\n",
    "        t = F.relu(self.fc3(t))\n",
    "\n",
    "        return t\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, row_size):\n",
    "        \"\"\"\n",
    "        Write your custom initialization sequence here.\n",
    "        This can include loading models from file.\n",
    "        \"\"\"\n",
    "        self.tn = TraderNetwork(row_size).double()        \n",
    "        self.tn.load_state_dict(torch.load(\"./example_model.pt\"))        \n",
    "        self.tn.eval()\n",
    "\n",
    "    def step(self, row):\n",
    "        \"\"\"\n",
    "        Make a decision to be executed @ the open of the next timestep. \n",
    "\n",
    "        row is a numpy array with the same format as the training data\n",
    "\n",
    "        Return a tuple (Action, fraction). Fraction means different \n",
    "        things for different actions...\n",
    "        \n",
    "        Action.BUY:  represents fraction of cash to spend on purchase \n",
    "        Action.SELL: represents fraction of owned shares to sell \n",
    "        Action.HOLD: value ignored.\n",
    "\n",
    "        See the code below on how to return\n",
    "        \"\"\"\n",
    "\n",
    "        t = torch.tensor(row)\n",
    "        choice = torch.argmax(self.tn(t).squeeze(0)).item()\n",
    "\n",
    "        # The plan was to never have to use constants...\n",
    "        # Yeah, we're assuming consistency in buy=0, sell=1, and hold=2\n",
    "        if choice == 0:\n",
    "            return (Action.BUY, 1)\n",
    "        elif choice == 1:\n",
    "            return (Action.SELL, 1)\n",
    "\n",
    "        return (Action.HOLD, 0)"
   ]
  },
  {
   "source": [
    "### Example main.py\n",
    "\n",
    "I call this an \"example\", but literally this is what your main.py file should contain. (You might need to slightly change if if you mess with your agent's constructor or step function interface)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from agent import Agent\n",
    "\n",
    "a = None\n",
    "for line in sys.stdin:\n",
    "    row = line.split(',')\n",
    "    row = np.array([float(x.strip()) for x in row])\n",
    "    if not a:\n",
    "        a = Agent(len(row))\n",
    "\n",
    "    res = a.step(row)\n",
    "    print(f\"{res[0].name} {res[1]}\")\n"
   ]
  },
  {
   "source": [
    "## Evaluate your model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from util import Evaluation\n",
    "\n",
    "agent = Agent(env.row_size)\n",
    "e = Evaluation(df, 1000, agent)\n",
    "\n",
    "print(\"---------------Evaluation Stats---------------\")\n",
    "print(f\"total:    {e.total()}\")\n",
    "print(f\"cash:     {e.cash}\")\n",
    "print(f\"n_shares: {e.n_shares}\")\n",
    "print(f\"n_buys:   {e.n_buys}\")\n",
    "print(f\"n_sells:  {e.n_sells}\")\n",
    "print(f\"n_holds:  {e.n_holds}\")\n",
    "\n",
    "plt.plot(e.account_values)\n",
    "plt.title(\"Account Value over Time\")\n",
    "plt.xlabel(\"Days\")\n",
    "plt.ylabel(\"Money (USD)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "Python 3.7.6 64-bit ('base': conda)",
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}